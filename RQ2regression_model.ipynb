{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "#Reading all the file\n",
    "yello_taxi_2023_01 = pd.read_parquet(\"Data/2023_01.parquet\")\n",
    "yello_taxi_2023_02 = pd.read_parquet(\"Data/2023_02.parquet\")\n",
    "yello_taxi_2023_03 = pd.read_parquet(\"Data/2023_03.parquet\")\n",
    "yello_taxi_2023_04 = pd.read_parquet(\"Data/2023_04.parquet\")\n",
    "yello_taxi_2023_05 = pd.read_parquet(\"Data/2023_05.parquet\")\n",
    "yello_taxi_2023_06 = pd.read_parquet(\"Data/2023_06.parquet\")\n",
    "yello_taxi_2023_07 = pd.read_parquet(\"Data/2023_07.parquet\")\n",
    "yello_taxi_2023_08 = pd.read_parquet(\"Data/2023_08.parquet\")\n",
    "yello_taxi_2023_09 = pd.read_parquet(\"Data/2023_09.parquet\")\n",
    "yello_taxi_2023_10 = pd.read_parquet(\"Data/2023_10.parquet\")\n",
    "yello_taxi_2023_11 = pd.read_parquet(\"Data/2023_11.parquet\")\n",
    "yello_taxi_2023_12 = pd.read_parquet(\"Data/2023_12.parquet\")\n",
    "\n",
    "yello_taxi_2024_01 = pd.read_parquet(\"Data/2024_01.parquet\")\n",
    "yello_taxi_2024_02 = pd.read_parquet(\"Data/2024_02.parquet\")\n",
    "yello_taxi_2024_03 = pd.read_parquet(\"Data/2024_03.parquet\")\n",
    "yello_taxi_2024_04 = pd.read_parquet(\"Data/2024_04.parquet\")\n",
    "yello_taxi_2024_05 = pd.read_parquet(\"Data/2024_05.parquet\")\n",
    "yello_taxi_2024_06 = pd.read_parquet(\"Data/2024_06.parquet\")\n",
    "yello_taxi_2024_07 = pd.read_parquet(\"Data/2024_07.parquet\")\n",
    "yello_taxi_2024_08 = pd.read_parquet(\"Data/2024_08.parquet\")\n",
    "yello_taxi_2024_09 = pd.read_parquet(\"Data/2024_09.parquet\")\n",
    "yello_taxi_2024_10 = pd.read_parquet(\"Data/2024_10.parquet\")\n",
    "yello_taxi_2024_11 = pd.read_parquet(\"Data/2024_11.parquet\")\n",
    "\n",
    "\n",
    "sum_dataset = pd.concat([yello_taxi_2023_01,yello_taxi_2023_02,yello_taxi_2023_03,\n",
    "            yello_taxi_2023_04,yello_taxi_2023_05,yello_taxi_2023_06,\n",
    "            yello_taxi_2023_07,yello_taxi_2023_08,yello_taxi_2023_09, \n",
    "            yello_taxi_2023_10, yello_taxi_2023_11, yello_taxi_2023_12,\n",
    "            yello_taxi_2024_01, yello_taxi_2024_02, yello_taxi_2024_03,\n",
    "            yello_taxi_2024_04, yello_taxi_2024_05, yello_taxi_2024_06,\n",
    "            yello_taxi_2024_07, yello_taxi_2024_08, yello_taxi_2024_09, \n",
    "            yello_taxi_2024_10, yello_taxi_2024_11\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 初始数据量: 75811575\n",
      "[2] 基础过滤后: 67854154\n",
      "[3] 时间过滤后: 67851501\n",
      "[4] 速度过滤后: 67781859\n",
      "[5] 最终数据量: 67781856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"dropoff_hour\"] = final_df[\"tpep_dropoff_datetime\"].dt.hour\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['time_sin'] = np.sin(2 * np.pi * final_df[\"dropoff_hour\"] / 24)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['time_cos'] = np.cos(2 * np.pi * final_df[\"dropoff_hour\"] / 24)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"time_angle\"] = np.arctan2(final_df[\"time_sin\"], final_df[\"time_cos\"])\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['is_early_morning'] = final_df[\"dropoff_hour\"].between(4, 6).astype(int)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['is_rush_hour'] = final_df[\"dropoff_hour\"].isin([8, 17, 18,21,22]).astype(int)\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"dropoff_dayofweek\"] = final_df[\"tpep_dropoff_datetime\"].dt.dayofweek\n",
      "/var/folders/ms/kdx178sn0hn6k06q5wb3d6km0000gn/T/ipykernel_33721/1072556880.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"is_weekend\"] = final_df[\"dropoff_dayofweek\"].isin([5, 6]).astype(int)\n"
     ]
    }
   ],
   "source": [
    "def safe_preprocess(df):\n",
    "    # 保留原始数据副本\n",
    "    raw_df = df\n",
    "    \n",
    "    # 逐步过滤（添加调试输出）\n",
    "    print(\"\\n[1] 初始数据量:\", len(raw_df))\n",
    "    \n",
    "    # 第一轮过滤：基础有效性过滤\n",
    "    cond = (\n",
    "        raw_df[\"passenger_count\"].between(1, 6) &\n",
    "        raw_df[\"trip_distance\"].between(0.1, 100) &\n",
    "        (raw_df[\"total_amount\"] > 0) &\n",
    "        (raw_df[\"tip_amount\"] >= 0) &\n",
    "        raw_df[\"tpep_pickup_datetime\"].notna() &\n",
    "        raw_df[\"tpep_dropoff_datetime\"].notna()\n",
    "    )\n",
    "    stage1 = raw_df[cond]\n",
    "    print(\"[2] 基础过滤后:\", len(stage1))\n",
    "    \n",
    "    # 时间逻辑过滤\n",
    "    time_cond = (stage1[\"tpep_dropoff_datetime\"] > stage1[\"tpep_pickup_datetime\"])\n",
    "    stage1 = stage1[time_cond]\n",
    "    print(\"[3] 时间过滤后:\", len(stage1))\n",
    "    \n",
    "    # 计算时间相关特征\n",
    "    stage1[\"trip_duration\"] = (stage1[\"tpep_dropoff_datetime\"] - stage1[\"tpep_pickup_datetime\"]).dt.total_seconds() / 3600\n",
    "    stage1[\"speed_kmh\"] = stage1[\"trip_distance\"] / stage1[\"trip_duration\"]\n",
    "    \n",
    "    # 速度过滤（放宽限制）\n",
    "    speed_cond = stage1[\"speed_kmh\"].between(0.5, 150)  # 包含低速和高速公路速度\n",
    "    stage1 = stage1[speed_cond]\n",
    "    print(\"[4] 速度过滤后:\", len(stage1))\n",
    "    \n",
    "    # 小费比例计算与过滤\n",
    "    stage1[\"tip_ratio\"] = stage1[\"tip_amount\"] / stage1[\"total_amount\"]\n",
    "    ratio_cond = stage1[\"tip_ratio\"].between(0, 1)  # 允许100%小费\n",
    "    final_df = stage1[ratio_cond]\n",
    "    print(\"[5] 最终数据量:\", len(final_df))\n",
    "    \n",
    "    # 时间特征（确保列存在）\n",
    "    final_df[\"dropoff_hour\"] = final_df[\"tpep_dropoff_datetime\"].dt.hour\n",
    "    # 生成时间周期特征（正弦+余弦编码）\n",
    "    final_df['time_sin'] = np.sin(2 * np.pi * final_df[\"dropoff_hour\"] / 24)\n",
    "    final_df['time_cos'] = np.cos(2 * np.pi * final_df[\"dropoff_hour\"] / 24)\n",
    "    final_df[\"time_angle\"] = np.arctan2(final_df[\"time_sin\"], final_df[\"time_cos\"])\n",
    "    # 生成其他时间相关特征\n",
    "    final_df['is_early_morning'] = final_df[\"dropoff_hour\"].between(4, 6).astype(int)\n",
    "    final_df['is_rush_hour'] = final_df[\"dropoff_hour\"].isin([8, 17, 18,21,22]).astype(int)\n",
    "    \n",
    "    final_df[\"dropoff_dayofweek\"] = final_df[\"tpep_dropoff_datetime\"].dt.dayofweek\n",
    "    final_df[\"is_weekend\"] = final_df[\"dropoff_dayofweek\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# 正确调用方式：从原始数据开始处理\n",
    "positive_pay_sum_dataset = safe_preprocess(sum_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           6.901186\n",
      "1          10.448549\n",
      "2          11.811765\n",
      "4           7.920000\n",
      "5           8.975610\n",
      "             ...    \n",
      "3272690     8.648649\n",
      "3272691     8.221860\n",
      "3272692     7.855787\n",
      "3272693     9.214854\n",
      "3272694     8.411683\n",
      "Name: speed_kmh, Length: 67781856, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(positive_pay_sum_dataset[\"speed_kmh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列出考虑因素和特征，以及裁剪数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passenger_count     1084509\n",
      "trip_distance       1084509\n",
      "speed_kmh           1084509\n",
      "time_sin            1084509\n",
      "time_cos            1084509\n",
      "is_early_morning    1084509\n",
      "is_rush_hour        1084509\n",
      "time_angle          1084509\n",
      "is_weekend          1084509\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 基础特征列表（不包含需要编码的类别特征）\n",
    "base_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"speed_kmh\",\n",
    "    \"time_sin\",\n",
    "    'time_cos',\n",
    "    'is_early_morning',\n",
    "    'is_rush_hour',\n",
    "    \"time_angle\",\n",
    "    \"is_weekend\"       # 已经是二值特征\n",
    "]\n",
    "\n",
    "# 原始数据获取（包含需要编码特征）\n",
    "processed_df = positive_pay_sum_dataset[base_features + [\"tip_ratio\"]].copy()\n",
    "\n",
    "\n",
    "# 安全抽样（先抽样再划分数据集）\n",
    "sample_df = processed_df.sample(frac=0.02)\n",
    "\n",
    "# 划分数据集（必须在此步骤之后进行标准化）\n",
    "X = sample_df[base_features]\n",
    "y = sample_df[\"tip_ratio\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 创建数据副本避免修改原始数据\n",
    "X_train_processed = X_train\n",
    "X_test_processed = X_test\n",
    "\n",
    "\n",
    "print(X_train_processed.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型评估函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_test, y_pred),\n",
    "        \"R2\": r2_score(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0632\n",
      "MSE: 0.0055\n",
      "R²: 0.0056\n"
     ]
    }
   ],
   "source": [
    "# 线性回归\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "#评估模型\n",
    "lr_metrics = evaluate_model(lr_model, X_test_processed, y_test)\n",
    "print(f\"MAE: {lr_metrics['MAE']:.4f}\")\n",
    "print(f\"MSE: {lr_metrics['MSE']:.4f}\")\n",
    "print(f\"R²: {lr_metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightBGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1084509, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 0.121620\n",
      "MAE: 0.0625\n",
      "MSE: 0.0054\n",
      "R²: 0.0191\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "LBGM_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    subsample=0.7,\n",
    "    reg_alpha=0.1,              # L1正则化\n",
    "    reg_lambda=0.1,             # L2正则化\n",
    "    random_state=42\n",
    ")\n",
    "LBGM_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "#评估模型\n",
    "LGBM_metrics = evaluate_model(LBGM_model, X_test_processed, y_test)\n",
    "print(f\"MAE: {LGBM_metrics['MAE']:.4f}\")\n",
    "print(f\"MSE: {LGBM_metrics['MSE']:.4f}\")\n",
    "print(f\"R²: {LGBM_metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0656\n",
      "MSE: 0.0065\n",
      "R²: -0.1736\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_model.fit(X_train_processed, y_train)\n",
    "\n",
    "#评估模型\n",
    "knn_metrics = evaluate_model(knn_model, X_test_processed, y_test)\n",
    "print(f\"MAE: {knn_metrics['MAE']:.4f}\")\n",
    "print(f\"MSE: {knn_metrics['MSE']:.4f}\")\n",
    "print(f\"R²: {knn_metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.0648\n",
      "MSE: 0.0062\n",
      "R²: -0.1122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "#评估模型\n",
    "knn_metrics = evaluate_model(rf_model, X_test_processed, y_test)\n",
    "print(f\"MAE: {knn_metrics['MAE']:.4f}\")\n",
    "print(f\"MSE: {knn_metrics['MSE']:.4f}\")\n",
    "print(f\"R²: {knn_metrics['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# ====================\n",
    "# 设备配置\n",
    "# ====================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================\n",
    "# 高级数据预处理\n",
    "# ====================\n",
    "# 假设原始数据包含以下特征：\n",
    "# ['trip_distance', 'speed_kmh', 'dropoff_hour', 'period_Morning_Rush', ...]\n",
    "\n",
    "# 定义特征处理管道\n",
    "numeric_features = ['trip_distance', 'speed_kmh', 'dropoff_hour']\n",
    "categorical_features = [col for col in X_train.columns if 'period_' in col]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    # 数值特征处理管道\n",
    "    ('numeric', \n",
    "     Pipeline([\n",
    "        ('scaler', RobustScaler()),\n",
    "        ('power', PowerTransformer(method='yeo-johnson'))\n",
    "     ]),  # 注意这里不需要指定列名\n",
    "     numeric_features),  # 列名在此处指定\n",
    "    \n",
    "    # 分类特征直通\n",
    "    ('categorical', \n",
    "     'passthrough', \n",
    "     categorical_features)\n",
    "])\n",
    "\n",
    "# 应用预处理\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# 添加交互特征\n",
    "X_train_processed = pd.DataFrame(X_train_processed, \n",
    "                                columns=numeric_features + categorical_features)\n",
    "X_train_processed['speed_hour'] = X_train_processed['speed_kmh'] * X_train_processed['dropoff_hour']\n",
    "\n",
    "X_test_processed = pd.DataFrame(X_test_processed,\n",
    "                               columns=numeric_features + categorical_features)\n",
    "X_test_processed['speed_hour'] = X_test_processed['speed_kmh'] * X_test_processed['dropoff_hour']\n",
    "\n",
    "# 转换目标变量（可选）\n",
    "y_train_trans = np.log(y_train / (1 - y_train + 1e-8))  # logit变换\n",
    "y_test_trans = np.log(y_test / (1 - y_test + 1e-8))\n",
    "\n",
    "# ====================\n",
    "# 增强模型架构\n",
    "# ====================\n",
    "class EnhancedSVR(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, C=1.0, epsilon=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # 初始化参数\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    def custom_loss(self, pred, target):\n",
    "        errors = target - pred\n",
    "        quadratic = torch.where(\n",
    "            torch.abs(errors) < self.epsilon,\n",
    "            0.5 * (errors**2),\n",
    "            self.epsilon * torch.abs(errors) - 0.5 * (self.epsilon**2)\n",
    "        )\n",
    "        # 所有参数的L2正则化\n",
    "        l2_reg = sum(torch.sum(param**2) for param in self.parameters()) / (2 * self.C)\n",
    "        return torch.mean(quadratic) + l2_reg\n",
    "\n",
    "# ====================\n",
    "# 数据转换\n",
    "# ====================\n",
    "input_dim = X_train_processed.shape[1]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_processed.values, \n",
    "                             dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.tensor(y_train_trans.values.reshape(-1, 1),\n",
    "                             dtype=torch.float32, device=device)\n",
    "X_test_tensor = torch.tensor(X_test_processed.values,\n",
    "                            dtype=torch.float32, device=device)\n",
    "\n",
    "# ====================\n",
    "# 模型初始化\n",
    "# ====================\n",
    "model = EnhancedSVR(input_dim, hidden_dim=128, C=0.1, epsilon=0.05).to(device)\n",
    "\n",
    "# ====================\n",
    "# 优化配置\n",
    "# ====================\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.hidden.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.output.parameters(), 'lr': 1e-2}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# 方案1：使用 CyclicLR（适合batch级别调度）\n",
    "scheduler = optim.lr_scheduler.CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=1e-4,\n",
    "    max_lr=1e-2,\n",
    "    step_size_up=500,       # 每个周期上升步数\n",
    "    step_size_down=500,     # 每个周期下降步数\n",
    "    cycle_momentum=False,\n",
    "    mode='triangular2'      # 更稳定的学习率变化\n",
    ")\n",
    "\n",
    "# ====================\n",
    "# 改进训练循环\n",
    "# ====================\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "patience = 50\n",
    "min_delta = 0.001\n",
    "history = {'train_loss': [], 'val_mae': [], 'val_r2': []}\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 创建DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_mae': [], 'val_r2': []}\n",
    "\n",
    "for epoch in range(3000):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Batch训练\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(X_batch)\n",
    "        loss = model.custom_loss(preds, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 每个batch更新学习率\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # 计算平均epoch loss\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # 验证与早停\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(X_test_tensor).cpu().numpy()\n",
    "        test_mae = mean_absolute_error(y_test_trans, test_pred)\n",
    "        test_r2 = r2_score(y_test_trans, test_pred)\n",
    "        \n",
    "        if avg_epoch_loss < (best_loss - min_delta):\n",
    "            best_loss = avg_epoch_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    \n",
    "    # 打印进度\n",
    "    if epoch % 10 == 0:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch:04d} | Loss: {avg_epoch_loss:.4f} | \"\n",
    "              f\"LR: {current_lr:.2e} | \"\n",
    "              f\"Test MAE: {test_mae:.4f} | \"\n",
    "              f\"Test R²: {test_r2:.4f}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 最终评估与结果转换\n",
    "# ====================\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 转换预测结果回原始比例\n",
    "    test_pred = torch.sigmoid(torch.tensor(test_pred)).numpy()\n",
    "    \n",
    "    # 计算最终指标\n",
    "    final_mae = mean_absolute_error(y_test, test_pred)\n",
    "    final_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    print(\"\\n=== 最终测试性能 ===\")\n",
    "    print(f\"MAE: {final_mae:.4f}\")\n",
    "    print(f\"R²: {final_r2:.4f}\")\n",
    "\n",
    "# ====================\n",
    "# 特征重要性分析（改进版）\n",
    "# ====================\n",
    "# 获取中间层激活值\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = input[0].detach()\n",
    "    return hook\n",
    "\n",
    "model.hidden[0].register_forward_hook(get_activation('hidden'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(X_test_tensor[:1000])\n",
    "    hidden_activation = activation['hidden'].cpu().numpy()\n",
    "\n",
    "# 计算特征重要性\n",
    "feature_importance = np.abs(hidden_activation).mean(axis=0)\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "print(\"\\n隐藏层特征重要性（Top 10）：\")\n",
    "for idx in sorted_idx[:10]:\n",
    "    print(f\"Feature {idx}: {feature_importance[idx]:.4f}\")\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance)\n",
    "plt.title(\"Hidden Layer Feature Importance\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Mean Absolute Activation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
