{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059dce22",
   "metadata": {},
   "source": [
    "### Read data from the combined parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "336221cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc308b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   VendorID | tpep_pickup_datetime   | tpep_dropoff_datetime   |   passenger_count |   trip_distance |   RatecodeID | store_and_fwd_flag   |   PULocationID |   DOLocationID |   payment_type |   fare_amount |   extra |   mta_tax |   tip_amount |   tolls_amount |   improvement_surcharge |   total_amount |   congestion_surcharge |   airport_fee |   Airport_fee |\n",
      "|--------:|-----------:|:-----------------------|:------------------------|------------------:|----------------:|-------------:|:---------------------|---------------:|---------------:|---------------:|--------------:|--------:|----------:|-------------:|---------------:|------------------------:|---------------:|-----------------------:|--------------:|--------------:|\n",
      "| 2790731 |          2 | 2023-01-29 17:52:02    | 2023-01-29 17:56:43     |                 1 |            1.17 |            1 | N                    |            262 |             74 |              2 |           7.2 |     0   |       0.5 |         0    |              0 |                       1 |          11.2  |                    2.5 |             0 |           nan |\n",
      "|  666153 |          1 | 2023-01-08 15:57:24    | 2023-01-08 16:02:47     |                 1 |            0.9  |            1 | N                    |            229 |            237 |              2 |           6.5 |     2.5 |       0.5 |         0    |              0 |                       1 |          10.5  |                    2.5 |             0 |           nan |\n",
      "| 1985683 |          2 | 2023-01-21 19:38:01    | 2023-01-21 19:45:02     |                 1 |            0.95 |            1 | N                    |             45 |            261 |              1 |           7.9 |     0   |       0.5 |         2.38 |              0 |                       1 |          14.28 |                    2.5 |             0 |           nan |\n",
      "| 2154231 |          2 | 2023-01-23 16:07:31    | 2023-01-23 16:26:46     |                 5 |            0.88 |            1 | N                    |            237 |            141 |              1 |          16.3 |     2.5 |       0.5 |         1.5  |              0 |                       1 |          24.3  |                    2.5 |             0 |           nan |\n",
      "| 2493619 |          2 | 2023-01-26 21:21:08    | 2023-01-26 21:24:48     |                 2 |            1.03 |            1 | N                    |            229 |            140 |              1 |           6.5 |     1   |       0.5 |         2.3  |              0 |                       1 |          13.8  |                    2.5 |             0 |           nan |\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_parquet(\"tripdata_combined.parquet\").sample(frac=0.30, random_state=42) # Sample 10% of points to save on storage\n",
    "df = pd.read_parquet(\"tripdata_combined.parquet\").iloc[:,:-3]\n",
    "print(df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39dbbb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "VendorID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tpep_pickup_datetime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "tpep_dropoff_datetime",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "passenger_count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "trip_distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RatecodeID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PULocationID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DOLocationID",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "payment_type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "fare_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "extra",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mta_tax",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tip_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tolls_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "improvement_surcharge",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_amount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "congestion_surcharge",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "airport_fee",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Airport_fee",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c95ebaf1-f5fb-49ac-bbca-d9e24c01c1d3",
       "rows": [
        [
         "count",
         "22719358.0",
         "22719358",
         "22719358",
         "21227487.0",
         "22719358.0",
         "21227487.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "22719358.0",
         "21227487.0",
         "968300.0",
         "20259187.0"
        ],
        [
         "mean",
         "1.747662543985618",
         "2023-12-15 20:51:24.882377",
         "2023-12-15 21:08:48.967704",
         "1.352563777332663",
         "3.4168078239710815",
         "1.9634030632076231",
         "164.7457833535613",
         "163.73031478266245",
         "1.120786863783739",
         "19.874788309599264",
         "1.5074837105872396",
         "0.4948570963140766",
         "3.4489914481738495",
         "0.5913178233293396",
         "0.9962025071307232",
         "28.799243334252747",
         "2.3054068882482417",
         "0.10979035422906123",
         "0.1516154868406121"
        ],
        [
         "min",
         "1.0",
         "2001-01-01 00:08:31",
         "2001-01-01 01:11:09",
         "0.0",
         "0.0",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         "-48.91",
         "-3.0",
         "-0.5",
         "0.0",
         "0.0",
         "-1.0",
         "0.01",
         "-2.5",
         "0.0",
         "0.0"
        ],
        [
         "25%",
         "1.0",
         "2023-06-22 16:45:06.250000",
         "2023-06-22 17:07:31.250000",
         "1.0",
         "1.04",
         "1.0",
         "132.0",
         "113.0",
         "1.0",
         "9.3",
         "0.0",
         "0.5",
         "0.0",
         "0.0",
         "1.0",
         "15.96",
         "2.5",
         "0.0",
         "0.0"
        ],
        [
         "50%",
         "2.0",
         "2023-12-13 23:41:00",
         "2023-12-13 23:57:26",
         "1.0",
         "1.79",
         "1.0",
         "161.0",
         "162.0",
         "1.0",
         "13.5",
         "1.0",
         "0.5",
         "2.74",
         "0.0",
         "1.0",
         "21.0",
         "2.5",
         "0.0",
         "0.0"
        ],
        [
         "75%",
         "2.0",
         "2024-06-07 14:06:48",
         "2024-06-07 14:24:54.750000",
         "1.0",
         "3.4",
         "1.0",
         "233.0",
         "234.0",
         "1.0",
         "22.6",
         "2.5",
         "0.5",
         "4.34",
         "0.0",
         "1.0",
         "30.72",
         "2.5",
         "0.0",
         "0.0"
        ],
        [
         "max",
         "6.0",
         "2026-06-26 23:53:12",
         "2026-06-27 20:59:10",
         "9.0",
         "99.91",
         "99.0",
         "265.0",
         "265.0",
         "4.0",
         "335544.44",
         "65.99",
         "53.16",
         "99.99",
         "355.0",
         "2.0",
         "335550.94",
         "2.75",
         "1.25",
         "1.75"
        ],
        [
         "std",
         "0.4375835616611254",
         null,
         null,
         "0.8599776472887968",
         "4.487399423949804",
         "9.265057038364986",
         "64.1639674561458",
         "69.73380487312406",
         "0.5466269333733855",
         "101.05152553730218",
         "1.8160231706752983",
         "0.05264071407737821",
         "4.024048411863445",
         "2.203933398809726",
         "0.06082272784799239",
         "101.98087623078919",
         "0.6697884353157307",
         "0.3538137224401933",
         "0.4869139539410573"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>22719358</td>\n",
       "      <td>22719358</td>\n",
       "      <td>2.122749e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.122749e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.271936e+07</td>\n",
       "      <td>2.122749e+07</td>\n",
       "      <td>968300.000000</td>\n",
       "      <td>2.025919e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.747663e+00</td>\n",
       "      <td>2023-12-15 20:51:24.882377</td>\n",
       "      <td>2023-12-15 21:08:48.967704</td>\n",
       "      <td>1.352564e+00</td>\n",
       "      <td>3.416808e+00</td>\n",
       "      <td>1.963403e+00</td>\n",
       "      <td>1.647458e+02</td>\n",
       "      <td>1.637303e+02</td>\n",
       "      <td>1.120787e+00</td>\n",
       "      <td>1.987479e+01</td>\n",
       "      <td>1.507484e+00</td>\n",
       "      <td>4.948571e-01</td>\n",
       "      <td>3.448991e+00</td>\n",
       "      <td>5.913178e-01</td>\n",
       "      <td>9.962025e-01</td>\n",
       "      <td>2.879924e+01</td>\n",
       "      <td>2.305407e+00</td>\n",
       "      <td>0.109790</td>\n",
       "      <td>1.516155e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2001-01-01 00:08:31</td>\n",
       "      <td>2001-01-01 01:11:09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.891000e+01</td>\n",
       "      <td>-3.000000e+00</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>-2.500000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2023-06-22 16:45:06.250000</td>\n",
       "      <td>2023-06-22 17:07:31.250000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.040000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.320000e+02</td>\n",
       "      <td>1.130000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.300000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.596000e+01</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2023-12-13 23:41:00</td>\n",
       "      <td>2023-12-13 23:57:26</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.790000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>1.620000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.350000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>2.740000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.100000e+01</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2024-06-07 14:06:48</td>\n",
       "      <td>2024-06-07 14:24:54.750000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.400000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.330000e+02</td>\n",
       "      <td>2.340000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.260000e+01</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>4.340000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.072000e+01</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2026-06-26 23:53:12</td>\n",
       "      <td>2026-06-27 20:59:10</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>9.991000e+01</td>\n",
       "      <td>9.900000e+01</td>\n",
       "      <td>2.650000e+02</td>\n",
       "      <td>2.650000e+02</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.355444e+05</td>\n",
       "      <td>6.599000e+01</td>\n",
       "      <td>5.316000e+01</td>\n",
       "      <td>9.999000e+01</td>\n",
       "      <td>3.550000e+02</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>3.355509e+05</td>\n",
       "      <td>2.750000e+00</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.750000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.375836e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.599776e-01</td>\n",
       "      <td>4.487399e+00</td>\n",
       "      <td>9.265057e+00</td>\n",
       "      <td>6.416397e+01</td>\n",
       "      <td>6.973380e+01</td>\n",
       "      <td>5.466269e-01</td>\n",
       "      <td>1.010515e+02</td>\n",
       "      <td>1.816023e+00</td>\n",
       "      <td>5.264071e-02</td>\n",
       "      <td>4.024048e+00</td>\n",
       "      <td>2.203933e+00</td>\n",
       "      <td>6.082273e-02</td>\n",
       "      <td>1.019809e+02</td>\n",
       "      <td>6.697884e-01</td>\n",
       "      <td>0.353814</td>\n",
       "      <td>4.869140e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
       "count  2.271936e+07                    22719358                    22719358   \n",
       "mean   1.747663e+00  2023-12-15 20:51:24.882377  2023-12-15 21:08:48.967704   \n",
       "min    1.000000e+00         2001-01-01 00:08:31         2001-01-01 01:11:09   \n",
       "25%    1.000000e+00  2023-06-22 16:45:06.250000  2023-06-22 17:07:31.250000   \n",
       "50%    2.000000e+00         2023-12-13 23:41:00         2023-12-13 23:57:26   \n",
       "75%    2.000000e+00         2024-06-07 14:06:48  2024-06-07 14:24:54.750000   \n",
       "max    6.000000e+00         2026-06-26 23:53:12         2026-06-27 20:59:10   \n",
       "std    4.375836e-01                         NaN                         NaN   \n",
       "\n",
       "       passenger_count  trip_distance    RatecodeID  PULocationID  \\\n",
       "count     2.122749e+07   2.271936e+07  2.122749e+07  2.271936e+07   \n",
       "mean      1.352564e+00   3.416808e+00  1.963403e+00  1.647458e+02   \n",
       "min       0.000000e+00   0.000000e+00  1.000000e+00  1.000000e+00   \n",
       "25%       1.000000e+00   1.040000e+00  1.000000e+00  1.320000e+02   \n",
       "50%       1.000000e+00   1.790000e+00  1.000000e+00  1.610000e+02   \n",
       "75%       1.000000e+00   3.400000e+00  1.000000e+00  2.330000e+02   \n",
       "max       9.000000e+00   9.991000e+01  9.900000e+01  2.650000e+02   \n",
       "std       8.599776e-01   4.487399e+00  9.265057e+00  6.416397e+01   \n",
       "\n",
       "       DOLocationID  payment_type   fare_amount         extra       mta_tax  \\\n",
       "count  2.271936e+07  2.271936e+07  2.271936e+07  2.271936e+07  2.271936e+07   \n",
       "mean   1.637303e+02  1.120787e+00  1.987479e+01  1.507484e+00  4.948571e-01   \n",
       "min    1.000000e+00  0.000000e+00 -4.891000e+01 -3.000000e+00 -5.000000e-01   \n",
       "25%    1.130000e+02  1.000000e+00  9.300000e+00  0.000000e+00  5.000000e-01   \n",
       "50%    1.620000e+02  1.000000e+00  1.350000e+01  1.000000e+00  5.000000e-01   \n",
       "75%    2.340000e+02  1.000000e+00  2.260000e+01  2.500000e+00  5.000000e-01   \n",
       "max    2.650000e+02  4.000000e+00  3.355444e+05  6.599000e+01  5.316000e+01   \n",
       "std    6.973380e+01  5.466269e-01  1.010515e+02  1.816023e+00  5.264071e-02   \n",
       "\n",
       "         tip_amount  tolls_amount  improvement_surcharge  total_amount  \\\n",
       "count  2.271936e+07  2.271936e+07           2.271936e+07  2.271936e+07   \n",
       "mean   3.448991e+00  5.913178e-01           9.962025e-01  2.879924e+01   \n",
       "min    0.000000e+00  0.000000e+00          -1.000000e+00  1.000000e-02   \n",
       "25%    0.000000e+00  0.000000e+00           1.000000e+00  1.596000e+01   \n",
       "50%    2.740000e+00  0.000000e+00           1.000000e+00  2.100000e+01   \n",
       "75%    4.340000e+00  0.000000e+00           1.000000e+00  3.072000e+01   \n",
       "max    9.999000e+01  3.550000e+02           2.000000e+00  3.355509e+05   \n",
       "std    4.024048e+00  2.203933e+00           6.082273e-02  1.019809e+02   \n",
       "\n",
       "       congestion_surcharge    airport_fee   Airport_fee  \n",
       "count          2.122749e+07  968300.000000  2.025919e+07  \n",
       "mean           2.305407e+00       0.109790  1.516155e-01  \n",
       "min           -2.500000e+00       0.000000  0.000000e+00  \n",
       "25%            2.500000e+00       0.000000  0.000000e+00  \n",
       "50%            2.500000e+00       0.000000  0.000000e+00  \n",
       "75%            2.500000e+00       0.000000  0.000000e+00  \n",
       "max            2.750000e+00       1.250000  1.750000e+00  \n",
       "std            6.697884e-01       0.353814  4.869140e-01  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69cceb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID : 0 null values\n",
      "tpep_pickup_datetime : 0 null values\n",
      "tpep_dropoff_datetime : 0 null values\n",
      "passenger_count : 1491871 null values\n",
      "trip_distance : 0 null values\n",
      "RatecodeID : 1491871 null values\n",
      "store_and_fwd_flag : 1491871 null values\n",
      "PULocationID : 0 null values\n",
      "DOLocationID : 0 null values\n",
      "payment_type : 0 null values\n",
      "fare_amount : 0 null values\n",
      "extra : 0 null values\n",
      "mta_tax : 0 null values\n",
      "tip_amount : 0 null values\n",
      "tolls_amount : 0 null values\n",
      "improvement_surcharge : 0 null values\n",
      "total_amount : 0 null values\n",
      "congestion_surcharge : 1491871 null values\n",
      "airport_fee : 21751058 null values\n",
      "Airport_fee : 2460171 null values\n"
     ]
    }
   ],
   "source": [
    "for col in df:\n",
    "    print(col, \":\", df[col].isna().sum(), \"null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62eec5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all cols with null values \n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c628855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all dates to float types, so that they can be scaled.\n",
    "df['tpep_pickup_datetime'] = df['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "df['tpep_dropoff_datetime'] = df['tpep_dropoff_datetime'].values.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b261419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   VendorID |   tpep_pickup_datetime |   tpep_dropoff_datetime |   trip_distance |   PULocationID |   DOLocationID |   payment_type |   fare_amount |   extra |   mta_tax |   tip_amount |   tolls_amount |   improvement_surcharge |   total_amount |\n",
      "|--------:|-----------:|-----------------------:|------------------------:|----------------:|---------------:|---------------:|---------------:|--------------:|--------:|----------:|-------------:|---------------:|------------------------:|---------------:|\n",
      "| 2790731 |          2 |            1.67501e+15 |             1.67502e+15 |            1.17 |            262 |             74 |              2 |           7.2 |     0   |       0.5 |         0    |              0 |                       1 |          11.2  |\n",
      "|  666153 |          1 |            1.67319e+15 |             1.67319e+15 |            0.9  |            229 |            237 |              2 |           6.5 |     2.5 |       0.5 |         0    |              0 |                       1 |          10.5  |\n",
      "| 1985683 |          2 |            1.67433e+15 |             1.67433e+15 |            0.95 |             45 |            261 |              1 |           7.9 |     0   |       0.5 |         2.38 |              0 |                       1 |          14.28 |\n",
      "| 2154231 |          2 |            1.67449e+15 |             1.67449e+15 |            0.88 |            237 |            141 |              1 |          16.3 |     2.5 |       0.5 |         1.5  |              0 |                       1 |          24.3  |\n",
      "| 2493619 |          2 |            1.67477e+15 |             1.67477e+15 |            1.03 |            229 |            140 |              1 |           6.5 |     1   |       0.5 |         2.3  |              0 |                       1 |          13.8  |\n"
     ]
    }
   ],
   "source": [
    "print(df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "751f3157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22719358, 14)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af381bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, scale, and split data\n",
    "\n",
    "target = 'tip_amount'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Split data for 80% training,, 20% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aab2a6",
   "metadata": {},
   "source": [
    "## Regression Using All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be39ec",
   "metadata": {},
   "source": [
    "### Test Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d47c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8df22ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models and their parameter grids\n",
    "models = {\n",
    "    'KNeighborsRegressor': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'param_grid': {\n",
    "            'model__n_neighbors': [3, 5, 7, 10],\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "        }\n",
    "    },\n",
    "    'KMeansRegression': {\n",
    "        'model': KMeans(),\n",
    "        'param_grid': {\n",
    "            'model__n_clusters': [5, 10, 15, 20],\n",
    "            'model__init': ['k-means++', 'random'],\n",
    "            'model__n_init': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'param_grid': {\n",
    "            'model__fit_intercept': [True, False],\n",
    "            'model__n_jobs': [-1]\n",
    "        }\n",
    "    },\n",
    "    'XGBoostRegressor': {\n",
    "        'model': XGBRegressor(random_state=42),\n",
    "        'param_grid': {\n",
    "            'model__n_estimators': [100, 200, 300, 400],\n",
    "            'model__max_depth': [3, 6, 9],\n",
    "            'model__learning_rate': [0.01, 0.1],\n",
    "            'model__subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'param_grid': {\n",
    "            'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__epsilon': [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest' : {\n",
    "        'model' : RandomForestRegressor(random_state=42),\n",
    "        'param_grid' : {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__max_depth': [None],\n",
    "            'model__min_samples_split': [5, 10],\n",
    "            'model__max_features': ['sqrt'],\n",
    "            'model__n_jobs': [-1] \n",
    "        }\n",
    "    },\n",
    "     'Neural Network': {\n",
    "        'model': MLPRegressor(random_state=42, early_stopping=True),\n",
    "        'param_grid': {\n",
    "            'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'model__activation': ['relu', 'tanh'],\n",
    "            'model__alpha': [0.0001, 0.001],  # L2 regularization\n",
    "            'model__learning_rate_init': [0.001, 0.01],\n",
    "            'model__batch_size': [32, 64],\n",
    "            'model__max_iter': [200, 500]\n",
    "        }\n",
    "    }, \n",
    "    # 'LightGBM': {\n",
    "    #     'model': LGBMRegressor(random_state=42, verbose=-1),  # verbose=-1 to suppress output\n",
    "    #     'param_grid': {\n",
    "    #         'model__n_estimators': [100, 200],\n",
    "    #         'model__max_depth': [3, 5, 7],\n",
    "    #         'model__learning_rate': [0.01, 0.1],\n",
    "    #         'model__num_leaves': [31, 63],\n",
    "    #         'model__subsample': [1.0],\n",
    "    #         'model__colsample_bytree': [1.0],\n",
    "    #         'model__reg_alpha': [0, 0.1],  # L1 regularization\n",
    "    #         'model__reg_lambda': [0, 0.1],  # L2 regularization\n",
    "    #         'model__n_jobs': [-1]\n",
    "    #     }\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa90851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning KNeighborsRegressor ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for KNeighborsRegressor: {'model__algorithm': 'auto', 'model__n_neighbors': 5, 'model__weights': 'distance'}\n",
      "\n",
      "=== Tuning KMeansRegression ===\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters for KMeansRegression: {'model__init': 'k-means++', 'model__n_clusters': 5, 'model__n_init': 5}\n",
      "\n",
      "=== Tuning LinearRegression ===\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters for LinearRegression: {'model__fit_intercept': True, 'model__n_jobs': -1}\n",
      "\n",
      "=== Tuning XGBoostRegressor ===\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters for XGBoostRegressor: {'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__n_estimators': 400, 'model__subsample': 0.8}\n",
      "\n",
      "=== Tuning SVR ===\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best parameters for SVR: {'model__C': 1, 'model__epsilon': 0.1, 'model__kernel': 'linear'}\n",
      "\n",
      "=== Tuning Random Forest ===\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters for Random Forest: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_split': 5, 'model__n_estimators': 200, 'model__n_jobs': -1}\n",
      "\n",
      "=== Tuning Neural Network ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best parameters for Neural Network: {'model__activation': 'relu', 'model__alpha': 0.001, 'model__batch_size': 32, 'model__hidden_layer_sizes': (50, 50), 'model__learning_rate_init': 0.01, 'model__max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "performances = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n=== Tuning {name} ===\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # Grid search with reduced CV folds for speed\n",
    "    search = GridSearchCV(\n",
    "        pipeline,\n",
    "        config['param_grid'],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    # Fit on a subset of data for demonstration\n",
    "    search.fit(X_train[:5000], y_train[:5000])  # Use full data in production\n",
    "    \n",
    "    # Store best parameters\n",
    "    best_params[name] = search.best_params_\n",
    "    \n",
    "    test_pred = search.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    test_evs = explained_variance_score(y_test, test_pred)\n",
    "\n",
    "    performances[name] = {\n",
    "        'RMSE': test_rmse,\n",
    "        'R²': test_r2,\n",
    "        'Mean Absolute Error': test_mae,\n",
    "        'Explained Variance Score': test_evs\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ead9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNeighborsRegressor Optimal Parameters Found ===\n",
      "  model__algorithm: auto\n",
      "  model__n_neighbors: 5\n",
      "  model__weights: distance\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.5171\n",
      "R²: 0.6188\n",
      "Mean Absolute Error: 1.2336\n",
      "Explained Variance Score: 0.6188\n",
      "\n",
      "=== KMeansRegression Optimal Parameters Found ===\n",
      "  model__init: k-means++\n",
      "  model__n_clusters: 5\n",
      "  model__n_init: 5\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 4.7747\n",
      "R²: -0.3718\n",
      "Mean Absolute Error: 2.9664\n",
      "Explained Variance Score: -0.1505\n",
      "\n",
      "=== LinearRegression Optimal Parameters Found ===\n",
      "  model__fit_intercept: True\n",
      "  model__n_jobs: -1\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 1.6806\n",
      "R²: 0.8301\n",
      "Mean Absolute Error: 0.2831\n",
      "Explained Variance Score: 0.8301\n",
      "\n",
      "=== XGBoostRegressor Optimal Parameters Found ===\n",
      "  model__learning_rate: 0.1\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 400\n",
      "  model__subsample: 0.8\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 1.5457\n",
      "R²: 0.8562\n",
      "Mean Absolute Error: 0.5825\n",
      "Explained Variance Score: 0.8563\n",
      "\n",
      "=== SVR Optimal Parameters Found ===\n",
      "  model__C: 1\n",
      "  model__epsilon: 0.1\n",
      "  model__kernel: linear\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 1.8720\n",
      "R²: 0.7891\n",
      "Mean Absolute Error: 0.2409\n",
      "Explained Variance Score: 0.7892\n",
      "\n",
      "=== Random Forest Optimal Parameters Found ===\n",
      "  model__max_depth: None\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_split: 5\n",
      "  model__n_estimators: 200\n",
      "  model__n_jobs: -1\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 1.9029\n",
      "R²: 0.7821\n",
      "Mean Absolute Error: 0.7953\n",
      "Explained Variance Score: 0.7821\n",
      "\n",
      "=== Neural Network Optimal Parameters Found ===\n",
      "  model__activation: relu\n",
      "  model__alpha: 0.001\n",
      "  model__batch_size: 32\n",
      "  model__hidden_layer_sizes: (50, 50)\n",
      "  model__learning_rate_init: 0.01\n",
      "  model__max_iter: 200\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 0.8251\n",
      "R²: 0.9590\n",
      "Mean Absolute Error: 0.2508\n",
      "Explained Variance Score: 0.9598\n"
     ]
    }
   ],
   "source": [
    "# Print all best parameters\n",
    "for name, params in best_params.items():\n",
    "    print(f\"\\n=== {name} Optimal Parameters Found ===\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print()\n",
    "\n",
    "    # Output results\n",
    "    print(\"Test Set Performance: \")\n",
    "    for score in performances[name]:\n",
    "        print(f\"{score}: {performances[name][score]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5eb7a1",
   "metadata": {},
   "source": [
    "### Test Baseline Simple Model\n",
    "(Simply Guesses Average Tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c95a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Model Test Set Performance: \n",
      "RMSE: 4.0766\n",
      "R²: -0.0000\n",
      "MAE: 2.5986\n",
      "Explained Variance Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_parquet('tripdata_combined.parquet')\n",
    "baseline_val = np.average(all_data['tip_amount'])\n",
    "test_pred = [baseline_val] * len(y_test)\n",
    "\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_evs = explained_variance_score(y_test, test_pred)\n",
    "\n",
    "print(\"Simple Model Test Set Performance: \")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"R²: {test_r2:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "print(f\"Explained Variance Score: {test_evs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66e9ac",
   "metadata": {},
   "source": [
    "## Regression Using Payment Method, Taxi Arrival Time, and Trip Length Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca7cf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ebe329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   trip_distance |   payment_type |   trip_distance |   tip_amount |\n",
      "|--------:|----------------:|---------------:|----------------:|-------------:|\n",
      "| 2790731 |            1.17 |              2 |            1.17 |         0    |\n",
      "|  666153 |            0.9  |              2 |            0.9  |         0    |\n",
      "| 1985683 |            0.95 |              1 |            0.95 |         2.38 |\n",
      "| 2154231 |            0.88 |              1 |            0.88 |         1.5  |\n",
      "| 2493619 |            1.03 |              1 |            1.03 |         2.3  |\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"tripdata_combined.parquet\")[['trip_distance', 'payment_type', 'trip_distance', 'tip_amount']]\n",
    "print(df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b1e6130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "trip_distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "payment_type",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "trip_distance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tip_amount",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "89136f1c-a53c-486d-87e9-8fbcc1bddb66",
       "rows": [
        [
         "count",
         "23000000.0",
         "23000000.0",
         "23000000.0",
         "23000000.0"
        ],
        [
         "mean",
         "4.500602536521737",
         "1.1477652173913044",
         "4.500602536521737",
         "3.4105981782608685"
        ],
        [
         "std",
         "339.71325984978864",
         "0.6030052092856388",
         "339.71325984978864",
         "4.092717456129904"
        ],
        [
         "min",
         "0.0",
         "0.0",
         "0.0",
         "-330.88"
        ],
        [
         "25%",
         "1.03",
         "1.0",
         "1.03",
         "0.0"
        ],
        [
         "50%",
         "1.78",
         "1.0",
         "1.78",
         "2.72"
        ],
        [
         "75%",
         "3.4",
         "1.0",
         "3.4",
         "4.3"
        ],
        [
         "max",
         "398608.62",
         "5.0",
         "398608.62",
         "999.99"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.300000e+07</td>\n",
       "      <td>2.300000e+07</td>\n",
       "      <td>2.300000e+07</td>\n",
       "      <td>2.300000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.500603e+00</td>\n",
       "      <td>1.147765e+00</td>\n",
       "      <td>4.500603e+00</td>\n",
       "      <td>3.410598e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.397133e+02</td>\n",
       "      <td>6.030052e-01</td>\n",
       "      <td>3.397133e+02</td>\n",
       "      <td>4.092717e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.308800e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.030000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.030000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.780000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.780000e+00</td>\n",
       "      <td>2.720000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.400000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.400000e+00</td>\n",
       "      <td>4.300000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.986086e+05</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>3.986086e+05</td>\n",
       "      <td>9.999900e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_distance  payment_type  trip_distance    tip_amount\n",
       "count   2.300000e+07  2.300000e+07   2.300000e+07  2.300000e+07\n",
       "mean    4.500603e+00  1.147765e+00   4.500603e+00  3.410598e+00\n",
       "std     3.397133e+02  6.030052e-01   3.397133e+02  4.092717e+00\n",
       "min     0.000000e+00  0.000000e+00   0.000000e+00 -3.308800e+02\n",
       "25%     1.030000e+00  1.000000e+00   1.030000e+00  0.000000e+00\n",
       "50%     1.780000e+00  1.000000e+00   1.780000e+00  2.720000e+00\n",
       "75%     3.400000e+00  1.000000e+00   3.400000e+00  4.300000e+00\n",
       "max     3.986086e+05  5.000000e+00   3.986086e+05  9.999900e+02"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d7cf309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trip_distance : trip_distance    0\n",
      "trip_distance    0\n",
      "dtype: int64 null values\n",
      "payment_type : 0 null values\n",
      "trip_distance : trip_distance    0\n",
      "trip_distance    0\n",
      "dtype: int64 null values\n",
      "tip_amount : 0 null values\n"
     ]
    }
   ],
   "source": [
    "for col in df:\n",
    "    print(col, \":\", df[col].isna().sum(), \"null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc3ec815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all cols with null values \n",
    "df = df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b09ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730af79d",
   "metadata": {},
   "source": [
    "### Test Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9e9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee64dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create, scale, and split data\n",
    "target = 'tip_amount'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "\n",
    "# Split data for 80% training,, 20% test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04dc06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all models and their parameter grids\n",
    "models = {\n",
    "    'KNeighborsRegressor': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'param_grid': {\n",
    "            'model__n_neighbors': [3, 5, 7, 10],\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "        }\n",
    "    },\n",
    "    'KMeansRegression': {\n",
    "        'model': KMeans(),\n",
    "        'param_grid': {\n",
    "            'model__n_clusters': [5, 10, 15, 20],\n",
    "            'model__init': ['k-means++', 'random'],\n",
    "            'model__n_init': [5, 10]\n",
    "        }\n",
    "    },\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'param_grid': {\n",
    "            'model__fit_intercept': [True, False],\n",
    "            'model__n_jobs': [-1]\n",
    "        }\n",
    "    },\n",
    "    'XGBoostRegressor': {\n",
    "        'model': XGBRegressor(random_state=42),\n",
    "        'param_grid': {\n",
    "            'model__n_estimators': [100, 200, 300, 400],\n",
    "            'model__max_depth': [3, 6, 9],\n",
    "            'model__learning_rate': [0.01, 0.1],\n",
    "            'model__subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model': SVR(),\n",
    "        'param_grid': {\n",
    "            'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__epsilon': [0.01, 0.1]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest' : {\n",
    "        'model' : RandomForestRegressor(random_state=42),\n",
    "        'param_grid' : {\n",
    "            'model__n_estimators': [100, 200],\n",
    "            'model__max_depth': [None],\n",
    "            'model__min_samples_split': [5, 10],\n",
    "            'model__max_features': ['sqrt'],\n",
    "            'model__n_jobs': [-1] \n",
    "        }\n",
    "    },\n",
    "     'Neural Network': {\n",
    "        'model': MLPRegressor(random_state=42, early_stopping=True),\n",
    "        'param_grid': {\n",
    "            'model__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'model__activation': ['relu', 'tanh'],\n",
    "            'model__alpha': [0.0001, 0.001],  # L2 regularization\n",
    "            'model__learning_rate_init': [0.001, 0.01],\n",
    "            'model__batch_size': [32, 64],\n",
    "            'model__max_iter': [200, 500]\n",
    "        }\n",
    "    }, \n",
    "    # 'LightGBM': {\n",
    "    #     'model': LGBMRegressor(random_state=42, verbose=-1),  # verbose=-1 to suppress output\n",
    "    #     'param_grid': {\n",
    "    #         'model__n_estimators': [100, 200],\n",
    "    #         'model__max_depth': [3, 5, 7],\n",
    "    #         'model__learning_rate': [0.01, 0.1],\n",
    "    #         'model__num_leaves': [31, 63],\n",
    "    #         'model__subsample': [1.0],\n",
    "    #         'model__colsample_bytree': [1.0],\n",
    "    #         'model__reg_alpha': [0, 0.1],  # L1 regularization\n",
    "    #         'model__reg_lambda': [0, 0.1],  # L2 regularization\n",
    "    #         'model__n_jobs': [-1]\n",
    "    #     }\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3bbd407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Tuning KNeighborsRegressor ===\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters for KNeighborsRegressor: {'model__algorithm': 'ball_tree', 'model__n_neighbors': 10, 'model__weights': 'uniform'}\n",
      "\n",
      "=== Tuning KMeansRegression ===\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best parameters for KMeansRegression: {'model__init': 'random', 'model__n_clusters': 5, 'model__n_init': 5}\n",
      "\n",
      "=== Tuning LinearRegression ===\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters for LinearRegression: {'model__fit_intercept': True, 'model__n_jobs': -1}\n",
      "\n",
      "=== Tuning XGBoostRegressor ===\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters for XGBoostRegressor: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 400, 'model__subsample': 1.0}\n",
      "\n",
      "=== Tuning SVR ===\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best parameters for SVR: {'model__C': 1, 'model__epsilon': 0.1, 'model__kernel': 'rbf'}\n",
      "\n",
      "=== Tuning Random Forest ===\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best parameters for Random Forest: {'model__max_depth': None, 'model__max_features': 'sqrt', 'model__min_samples_split': 10, 'model__n_estimators': 200, 'model__n_jobs': -1}\n",
      "\n",
      "=== Tuning Neural Network ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best parameters for Neural Network: {'model__activation': 'tanh', 'model__alpha': 0.0001, 'model__batch_size': 64, 'model__hidden_layer_sizes': (50,), 'model__learning_rate_init': 0.001, 'model__max_iter': 200}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store best parameters\n",
    "best_params = {}\n",
    "performances = {}\n",
    "\n",
    "for name, config in models.items():\n",
    "    print(f\"\\n=== Tuning {name} ===\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', config['model'])\n",
    "    ])\n",
    "    \n",
    "    # Grid search with reduced CV folds for speed\n",
    "    search = GridSearchCV(\n",
    "        pipeline,\n",
    "        config['param_grid'],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "    \n",
    "    # Fit on a subset of data for demonstration\n",
    "    search.fit(X_train[:5000], y_train[:5000])  # Use full data in production\n",
    "    \n",
    "    # Store best parameters\n",
    "    best_params[name] = search.best_params_\n",
    "    \n",
    "    test_pred = search.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    test_evs = explained_variance_score(y_test, test_pred)\n",
    "\n",
    "    performances[name] = {\n",
    "        'RMSE': test_rmse,\n",
    "        'R²': test_r2,\n",
    "        'Mean Absolute Error': test_mae,\n",
    "        'Explained Variance Score': test_evs\n",
    "    }\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebbddf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KNeighborsRegressor Optimal Parameters Found ===\n",
      "  model__algorithm: ball_tree\n",
      "  model__n_neighbors: 10\n",
      "  model__weights: uniform\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.7579\n",
      "R²: 0.5423\n",
      "Mean Absolute Error: 1.3680\n",
      "Explained Variance Score: 0.5424\n",
      "\n",
      "=== KMeansRegression Optimal Parameters Found ===\n",
      "  model__init: random\n",
      "  model__n_clusters: 5\n",
      "  model__n_init: 5\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 4.4914\n",
      "R²: -0.2139\n",
      "Mean Absolute Error: 2.6052\n",
      "Explained Variance Score: -0.0633\n",
      "\n",
      "=== LinearRegression Optimal Parameters Found ===\n",
      "  model__fit_intercept: True\n",
      "  model__n_jobs: -1\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 136.8987\n",
      "R²: -1126.7464\n",
      "Mean Absolute Error: 2.4246\n",
      "Explained Variance Score: -1126.7353\n",
      "\n",
      "=== XGBoostRegressor Optimal Parameters Found ===\n",
      "  model__learning_rate: 0.01\n",
      "  model__max_depth: 3\n",
      "  model__n_estimators: 400\n",
      "  model__subsample: 1.0\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.6690\n",
      "R²: 0.5713\n",
      "Mean Absolute Error: 1.3470\n",
      "Explained Variance Score: 0.5714\n",
      "\n",
      "=== SVR Optimal Parameters Found ===\n",
      "  model__C: 1\n",
      "  model__epsilon: 0.1\n",
      "  model__kernel: rbf\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.7499\n",
      "R²: 0.5450\n",
      "Mean Absolute Error: 1.2399\n",
      "Explained Variance Score: 0.5454\n",
      "\n",
      "=== Random Forest Optimal Parameters Found ===\n",
      "  model__max_depth: None\n",
      "  model__max_features: sqrt\n",
      "  model__min_samples_split: 10\n",
      "  model__n_estimators: 200\n",
      "  model__n_jobs: -1\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.7985\n",
      "R²: 0.5287\n",
      "Mean Absolute Error: 1.4022\n",
      "Explained Variance Score: 0.5288\n",
      "\n",
      "=== Neural Network Optimal Parameters Found ===\n",
      "  model__activation: tanh\n",
      "  model__alpha: 0.0001\n",
      "  model__batch_size: 64\n",
      "  model__hidden_layer_sizes: (50,)\n",
      "  model__learning_rate_init: 0.001\n",
      "  model__max_iter: 200\n",
      "\n",
      "Test Set Performance: \n",
      "RMSE: 2.6600\n",
      "R²: 0.5742\n",
      "Mean Absolute Error: 1.3282\n",
      "Explained Variance Score: 0.5742\n"
     ]
    }
   ],
   "source": [
    "# Print all best parameters\n",
    "for name, params in best_params.items():\n",
    "    print(f\"\\n=== {name} Optimal Parameters Found ===\")\n",
    "    for param, value in params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print()\n",
    "\n",
    "    # Output results\n",
    "    print(\"Test Set Performance: \")\n",
    "    for score in performances[name]:\n",
    "        print(f\"{score}: {performances[name][score]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99d8e56",
   "metadata": {},
   "source": [
    "### Test Baseline Simple Model\n",
    "(Simply Guesses Average Tip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3197e124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Model Test Set Performance: \n",
      "RMSE: 4.0766\n",
      "R²: -0.0000\n",
      "MAE: 2.5986\n",
      "Explained Variance Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_parquet('tripdata_combined.parquet')\n",
    "baseline_val = np.average(all_data['tip_amount'])\n",
    "test_pred = [baseline_val] * len(y_test)\n",
    "\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "test_evs = explained_variance_score(y_test, test_pred)\n",
    "\n",
    "print(\"Simple Model Test Set Performance: \")\n",
    "print(f\"RMSE: {test_rmse:.4f}\")\n",
    "print(f\"R²: {test_r2:.4f}\")\n",
    "print(f\"MAE: {test_mae:.4f}\")\n",
    "print(f\"Explained Variance Score: {test_evs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505a57a5",
   "metadata": {},
   "source": [
    "## Use Best Model to Verify RQ2 Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "243eedda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b30f6794",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m mlp_regressor = Pipeline([\n\u001b[32m      2\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m'\u001b[39m, StandardScaler()),  \n\u001b[32m      3\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, MLPRegressor(\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     ))\n\u001b[32m     11\u001b[39m ])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Fit the model (replace with your actual data)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mmlp_regressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Save to a .pkl file\u001b[39;00m\n\u001b[32m     17\u001b[39m joblib.dump(mlp_regressor, \u001b[33m'\u001b[39m\u001b[33mkevin_mlp_regressor.pkl\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:654\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m    653\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:588\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    582\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    583\u001b[39m     step_idx=step_idx,\n\u001b[32m    584\u001b[39m     step_params=routed_params[name],\n\u001b[32m    585\u001b[39m     all_params=raw_params,\n\u001b[32m    586\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    600\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\joblib\\memory.py:312\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1551\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1551\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1553\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1554\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1555\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\base.py:921\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1016\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1013\u001b[39m         \u001b[38;5;28mself\u001b[39m.n_samples_seen_ += X.shape[\u001b[32m0\u001b[39m] - np.isnan(X).sum(axis=\u001b[32m0\u001b[39m)\n\u001b[32m   1015\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m         \u001b[38;5;28mself\u001b[39m.mean_, \u001b[38;5;28mself\u001b[39m.var_, \u001b[38;5;28mself\u001b[39m.n_samples_seen_ = \u001b[43m_incremental_mean_and_var\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_samples_seen_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.ptp(\u001b[38;5;28mself\u001b[39m.n_samples_seen_) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1097\u001b[39m, in \u001b[36m_incremental_mean_and_var\u001b[39m\u001b[34m(X, last_mean, last_variance, last_sample_count, sample_weight)\u001b[39m\n\u001b[32m   1095\u001b[39m     new_sum = _safe_accumulator_op(sum_op, X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m   1096\u001b[39m     n_samples = X.shape[\u001b[32m0\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     new_sample_count = n_samples - \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_nan_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1099\u001b[39m updated_sample_count = last_sample_count + new_sample_count\n\u001b[32m   1101\u001b[39m updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Py Torch\\Documents\\GitHub\\CabBoost\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "mlp_regressor = Pipeline([\n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('model', MLPRegressor(\n",
    "        activation='relu',\n",
    "        alpha=0.001,              \n",
    "        batch_size=32,\n",
    "        hidden_layer_sizes=(50, 50),  \n",
    "        learning_rate_init=0.01,\n",
    "        max_iter=200\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit the model (replace with your actual data)\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Save to a .pkl file\n",
    "joblib.dump(mlp_regressor, 'kevin_mlp_regressor.pkl')\n",
    "print(\"Model saved to kevin_mlp_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624a9bf",
   "metadata": {},
   "source": [
    "### To Test:\n",
    "\n",
    "To get a higher tip ratio:\n",
    "\n",
    "1. Avoid work late at night and early in the morning.\n",
    "(23:00 - 6:00)\n",
    "2. Since online orders can only be paid by credit card.\n",
    "Therefore get more online orders can get more tips.\n",
    "3. Try to get more orders for trips that are less than 30\n",
    "kilometers long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42e05312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "mlp_regressor = joblib.load('kevin_mlp_regressor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|         |   VendorID | tpep_pickup_datetime   | tpep_dropoff_datetime   |   trip_distance |   PULocationID |   DOLocationID |   payment_type |   fare_amount |   extra |   mta_tax |   tip_amount |   tolls_amount |   improvement_surcharge |   total_amount |\n",
      "|--------:|-----------:|:-----------------------|:------------------------|----------------:|---------------:|---------------:|---------------:|--------------:|--------:|----------:|-------------:|---------------:|------------------------:|---------------:|\n",
      "| 1409269 |          2 | 2023-05-13 13:14:30    | 2023-05-13 13:26:49     |            2.74 |             87 |             25 |              1 |         15.6  |     0   |       0.5 |         0.4  |              0 |                       1 |          20    |\n",
      "|  495957 |          2 | 2023-06-05 15:19:10    | 2023-06-06 00:00:00     |           17.3  |            132 |            163 |              1 |         70    |     0   |       0.5 |         7.58 |              0 |                       1 |          83.33 |\n",
      "| 2735062 |          2 | 2024-06-26 21:48:36    | 2024-06-26 22:14:56     |            1.74 |            234 |             48 |              1 |         22.6  |     1   |       0.5 |         5.52 |              0 |                       1 |          33.12 |\n",
      "| 3404752 |          1 | 2024-11-13 08:19:47    | 2024-11-13 09:17:47     |            6.8  |            177 |            125 |              0 |         44.26 |     0   |       0.5 |         0    |              0 |                       1 |          48.26 |\n",
      "| 2676877 |          2 | 2024-04-26 18:47:33    | 2024-04-26 18:59:46     |            1.62 |            231 |            113 |              1 |         12.8  |     2.5 |       0.5 |         3.86 |              0 |                       1 |          23.16 |\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "test_samples = pd.read_parquet(\"tripdata_combined.parquet\").sample(n=200, random_state=42).iloc[:,:-3]\n",
    "\n",
    "# Remove all cols with null values \n",
    "test_samples = test_samples.dropna(axis=1)\n",
    "\n",
    "print(test_samples.head().to_markdown())\n",
    "print(len(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14f738",
   "metadata": {},
   "source": [
    "Testing 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d587d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Py Torch\\AppData\\Local\\Temp\\ipykernel_104892\\80957425.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_late['tpep_pickup_datetime'] = test_late['tpep_pickup_datetime'].values.astype(np.float64)\n",
      "C:\\Users\\Py Torch\\AppData\\Local\\Temp\\ipykernel_104892\\80957425.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_late['tpep_dropoff_datetime'] = test_late['tpep_dropoff_datetime'].values.astype(np.float64)\n",
      "C:\\Users\\Py Torch\\AppData\\Local\\Temp\\ipykernel_104892\\80957425.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_early['tpep_pickup_datetime'] = test_early['tpep_pickup_datetime'].values.astype(np.float64)\n",
      "C:\\Users\\Py Torch\\AppData\\Local\\Temp\\ipykernel_104892\\80957425.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_early['tpep_dropoff_datetime'] = test_early['tpep_dropoff_datetime'].values.astype(np.float64)\n"
     ]
    }
   ],
   "source": [
    "test_late = test_samples[\n",
    "    (test_samples['tpep_pickup_datetime'].dt.hour >= 23) | \n",
    "    (test_samples['tpep_pickup_datetime'].dt.hour < 6)\n",
    "]\n",
    "\n",
    "test_early = test_samples[\n",
    "    ~(  # Invert the condition with NOT (~)\n",
    "        (test_samples['tpep_pickup_datetime'].dt.hour >= 23) | \n",
    "        (test_samples['tpep_pickup_datetime'].dt.hour < 6)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Convert all dates to float types, so that they can be scaled.\n",
    "test_late['tpep_pickup_datetime'] = test_late['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "test_late['tpep_dropoff_datetime'] = test_late['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "\n",
    "test_early['tpep_pickup_datetime'] = test_early['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "test_early['tpep_dropoff_datetime'] = test_early['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "\n",
    "\n",
    "print(test_late.shape[0], test_early.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf90ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID int64\n",
      "tpep_pickup_datetime float64\n",
      "tpep_dropoff_datetime float64\n",
      "trip_distance float64\n",
      "PULocationID int64\n",
      "DOLocationID int64\n",
      "payment_type int64\n",
      "fare_amount float64\n",
      "extra float64\n",
      "mta_tax float64\n",
      "tip_amount float64\n",
      "tolls_amount float64\n",
      "improvement_surcharge float64\n",
      "total_amount float64\n"
     ]
    }
   ],
   "source": [
    "for col in test_late:\n",
    "    print(col, test_late[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03fcf2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 8.955422955406398\n"
     ]
    }
   ],
   "source": [
    "test_late = test_late.drop(columns=['tip_amount'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(test_late)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=test_late.columns, index=test_late.index)\n",
    "\n",
    "late_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(late_predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e39740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 8.760839908103048\n"
     ]
    }
   ],
   "source": [
    "test_early = test_early.drop(columns=['tip_amount'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(test_early)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=test_early.columns, index=test_early.index)\n",
    "\n",
    "early_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(early_predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3223e94",
   "metadata": {},
   "source": [
    "Testing 2.\n",
    "\n",
    "Payment_type values:\n",
    "- 1=Credit Card\n",
    "- 2=Cash\n",
    "- 3=No charge\n",
    "- 4=Dispute\n",
    "- 5=Unknown\n",
    "- 6=Voided Trip\n",
    "\n",
    "[Source](https://medium.com/@liam.lim/nyc-yellow-taxi-trip-record-analysis-7eb389a0470c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50a3e08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 13\n"
     ]
    }
   ],
   "source": [
    "test_cc = test_samples[test_samples['payment_type']==1].drop(columns=['tip_amount'])\n",
    "test_no_cc = test_samples[test_samples['payment_type'] != 1].drop(columns=['tip_amount'])\n",
    "\n",
    "\n",
    "# Convert all dates to float types, so that they can be scaled.\n",
    "test_cc['tpep_pickup_datetime'] = test_cc['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "test_cc['tpep_dropoff_datetime'] = test_cc['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "test_no_cc['tpep_pickup_datetime'] = test_no_cc['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "test_no_cc['tpep_dropoff_datetime'] = test_no_cc['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "\n",
    "print(test_cc.shape[0], test_no_cc.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "691031a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 7.440410961165445\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(test_cc)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=test_cc.columns, index=test_cc.index)\n",
    "\n",
    "cc_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(cc_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0d69341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 5.040474825240105\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(test_no_cc)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=test_no_cc.columns, index=test_no_cc.index)\n",
    "\n",
    "no_cc_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(no_cc_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5764b",
   "metadata": {},
   "source": [
    "Testing 3.\n",
    "\n",
    "Note that 30 km is roughly 18.6411 miles, and our data stores trip distance in miles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f2b57ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 199\n"
     ]
    }
   ],
   "source": [
    "long_trips = test_samples[\n",
    "    test_samples['trip_distance'] >= 18.6411\n",
    "].drop(columns=['tip_amount'])\n",
    "\n",
    "short_trips = test_samples[\n",
    "    test_samples['trip_distance'] < 18.6411\n",
    "].drop(columns=['tip_amount'])\n",
    "\n",
    "# Convert datetime columns to float64 for scaling\n",
    "long_trips['tpep_pickup_datetime'] = long_trips['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "long_trips['tpep_dropoff_datetime'] = long_trips['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "\n",
    "short_trips['tpep_pickup_datetime'] = short_trips['tpep_pickup_datetime'].values.astype(np.float64)\n",
    "short_trips['tpep_dropoff_datetime'] = short_trips['tpep_dropoff_datetime'].values.astype(np.float64)\n",
    "\n",
    "print(long_trips.shape[0], short_trips.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70157a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 3.305815605862838\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(long_trips)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=long_trips.columns, index=long_trips.index)\n",
    "\n",
    "long_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(long_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94c4bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Tip Predicted: 8.716635970686118\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(short_trips)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=short_trips.columns, index=short_trips.index)\n",
    "\n",
    "short_predictions = mlp_regressor.predict(X_scaled)  \n",
    "print(\"Mean Tip Predicted:\", np.mean(short_predictions)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
